{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# decision tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# NN\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# so values can be viewed as scrollable element\n",
    "np.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcTotalProb(X_data_og):\n",
    "  X_data = X_data_og.copy()\n",
    "  # Calculate Probabilities\n",
    "  total_crashes = X_data.shape[0]\n",
    "  total_prob = []\n",
    "\n",
    "  hour_prob = {}\n",
    "  day_prob = {}\n",
    "  month_prob = {}\n",
    "\n",
    "  # probability for each hour\n",
    "  for hour in range(24):\n",
    "      occurrences_in_hour = X_data['Hour'].value_counts()[hour]\n",
    "      hour_prob[hour] = occurrences_in_hour/total_crashes\n",
    "\n",
    "  for day in range(1,8):\n",
    "      occurrences_in_day = X_data['Day'].value_counts()[day]\n",
    "      day_prob[day] = occurrences_in_day/total_crashes\n",
    "\n",
    "  months = X_data['Month'].unique()\n",
    "  for month in months:\n",
    "      occurrences_in_month = X_data['Month'].value_counts()[month]\n",
    "      month_prob[month] = occurrences_in_month/total_crashes\n",
    "      \n",
    "  # when calculating the total probability, the value gets extremely small due to multiplying\n",
    "      # probabilities. To reduce the effect of this we'll use log probabilities which shouldn't\n",
    "      # affect effectiveness of our product since we're simply comparing probabilities\n",
    "  def CalculateTotalProb(row):\n",
    "      probabilities = [hour_prob[row['Hour']],\n",
    "                      day_prob[row['Day']],\n",
    "                      month_prob[row['Month']]]\n",
    "      log_probabilities = np.log(probabilities)\n",
    "      log_combined_probability  = np.sum(log_probabilities)\n",
    "      return log_combined_probability * -1\n",
    "\n",
    "  X_data['total_prob'] = X_data.apply(CalculateTotalProb, axis=1)\n",
    "\n",
    "  return X_data['total_prob']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating total_prob\n",
    "\n",
    "Total probability is the probability of a given combination of travel details appearing in the crash database. The total_prob is therefore calculated manually for the training, testing, and validation splits to avoid data leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = \".\\\\modified_data\\\\cleaned_data.csv\"\n",
    "df = pd.read_csv(data_filepath)\n",
    "\n",
    "# splitting data: We will use a 80-20 split\n",
    "X_train, X_test = train_test_split(df, test_size=0.2, random_state=23)\n",
    "\n",
    "# Y_train is the probability of each value for its iven \n",
    "Y_train = CalcTotalProb(X_train)\n",
    "Y_test = CalcTotalProb(X_test)\n",
    "\n",
    "train, Xtest = train_test_split(df, test_size=0.3, random_state=23)\n",
    "Xtrain, Xval = train_test_split(train, test_size=0.25, random_state=23)\n",
    "\n",
    "Ytrain = CalcTotalProb(Xtrain)\n",
    "Ytest = CalcTotalProb(Xtest)\n",
    "Yval = CalcTotalProb(Xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Latitude and Longitude are not part of total_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model\n",
    "\n",
    "We determined through EDA that a simple model will not be sufficient to predict the probability type. We will instead use more complicated models (Decision trees, SVM). If those models don't work, we will increase complexity even more to random forest and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.00\n",
      "Testing MSE: 0.00\n",
      "Training R2: 1.00\n",
      "Test R2: 0.99\n"
     ]
    }
   ],
   "source": [
    "# create decision tree classifier\n",
    "clf = DecisionTreeRegressor(random_state=23)\n",
    "\n",
    "# train classifier\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate MSE and R2\n",
    "print(\"Training MSE: %.2f\" % (mean_squared_error(y_train_pred, Y_train)))\n",
    "print(\"Testing MSE: %.2f\" % (mean_squared_error(y_pred, Y_test)))\n",
    "\n",
    "r2 = r2_score(Y_train, y_train_pred)\n",
    "print('Training R2: %.2f' % r2)\n",
    "r2 = r2_score(Y_test, y_pred)\n",
    "print('Test R2: %.2f' % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These errors seem too good to be true so we will use the tougher \"train, test, validate\" split to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.00\n",
      "Testing MSE: 0.00\n",
      "Validation MSE: 0.00\n",
      "Training R2: 1.00\n",
      "Test R2: 0.99\n",
      "Validation R2: 0.99\n"
     ]
    }
   ],
   "source": [
    "# create decision tree classifier\n",
    "clf = DecisionTreeRegressor(random_state=23)\n",
    "\n",
    "# train classifier\n",
    "clf.fit(Xtrain, Ytrain)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_train_pred = clf.predict(Xtrain)\n",
    "y_test_pred = clf.predict(Xtest)\n",
    "y_val_pred = clf.predict(Xval)\n",
    "\n",
    "# Calculate MSE and R2\n",
    "print(\"Training MSE: %.2f\" % (mean_squared_error(y_train_pred, Ytrain)))\n",
    "print(\"Testing MSE: %.2f\" % (mean_squared_error(y_test_pred, Ytest)))\n",
    "print(\"Validation MSE: %.2f\" % (mean_squared_error(y_val_pred, Yval)))\n",
    "\n",
    "r2 = r2_score(Ytrain, y_train_pred)\n",
    "print('Training R2: %.2f' % r2)\n",
    "r2 = r2_score(Ytest, y_test_pred)\n",
    "print('Test R2: %.2f' % r2)\n",
    "r2 = r2_score(Yval, y_val_pred)\n",
    "print('Validation R2: %.2f' % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors remained the same and they're equally good for both training, testing, and validation so there's no overfitting. We will stop here for trees since we've achieved nearly perfect accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model\n",
    "\n",
    "We want to one-hot encode categorical data and try different kernels. We'll keep other parameters at their defaults unless this model seems more promising than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['Month', 'Day', 'Hour']\n",
    "\n",
    "df_svm = df.copy()\n",
    "\n",
    "df_svm = pd.get_dummies(df_svm, columns=cats)\n",
    "df_svm = df_svm.astype(float)\n",
    "\n",
    "# split data\n",
    "svm_train, svm_test = train_test_split(df_svm, test_size=0.2)\n",
    "X_svm_train, y_svm_train = svm_train.drop(columns=['total_prob']), svm_train['total_prob']\n",
    "X_svm_test, y_svm_test = svm_test.drop(columns=['total_prob']), svm_test['total_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel\n",
      "Training MSE: 0.01\n",
      "Testing MSE: 0.01\n",
      "Training R2: 1.00\n",
      "Test R2: -1.32\n",
      "\n",
      "rbf Kernel\n",
      "Training MSE: 0.00\n",
      "Testing MSE: 0.00\n",
      "Training R2: 1.00\n",
      "Test R2: -1.02\n"
     ]
    }
   ],
   "source": [
    "svc_li = SVR(kernel='linear')\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_svm_train)\n",
    "\n",
    "Z_svm_train = scaler.transform(X_svm_train)\n",
    "Z_svm_test = scaler.transform(X_svm_test)\n",
    "\n",
    "svc_li.fit(Z_svm_train, np.asarray(y_svm_train))\n",
    "\n",
    "print('Linear Kernel')\n",
    "# Calculate MSE and R2\n",
    "y_train_pred = svc_li.predict(Z_svm_train)\n",
    "y_test_pred = svc_li.predict(Z_svm_test)\n",
    "print(\"Training MSE: %.2f\" % (mean_squared_error(y_train_pred, y_svm_train)))\n",
    "print(\"Testing MSE: %.2f\" % (mean_squared_error(y_test_pred, y_svm_test)))\n",
    "\n",
    "r2 = r2_score(y_train_pred, y_train_pred)\n",
    "print('Training R2: %.2f' % r2)\n",
    "r2 = r2_score(y_test_pred, y_pred)\n",
    "print('Test R2: %.2f' % r2)\n",
    "\n",
    "svc_rbf = SVR(kernel='rbf')\n",
    "svc_rbf.fit(Z_svm_train, np.asarray(y_svm_train))\n",
    "\n",
    "print('')\n",
    "print('rbf Kernel')\n",
    "# Calculate MSE and R2\n",
    "y_train_pred = svc_rbf.predict(Z_svm_train)\n",
    "y_test_pred = svc_rbf.predict(Z_svm_test)\n",
    "print(\"Training MSE: %.2f\" % (mean_squared_error(y_train_pred, y_svm_train)))\n",
    "print(\"Testing MSE: %.2f\" % (mean_squared_error(y_test_pred, y_svm_test)))\n",
    "\n",
    "r2 = r2_score(y_train_pred, y_train_pred)\n",
    "print('Training R2: %.2f' % r2)\n",
    "r2 = r2_score(y_test_pred, y_pred)\n",
    "print('Test R2: %.2f' % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM is a poor model for this use case** as is evident by its negative R2 score. It does well in training for both kernels but loses all its performance when operating on the test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one hot encoding\n",
    "cats = ['Month', 'Day', 'Hour']\n",
    "X_train = pd.get_dummies(X_train, columns=cats)\n",
    "X_train = X_train.astype(float)\n",
    "X_test = pd.get_dummies(X_test, columns=cats)\n",
    "X_test = X_test.astype(float)\n",
    "# print(X)\n",
    "\n",
    "# normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_rescaled = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(data = X_train_rescaled, columns = X_train.columns)\n",
    "\n",
    "X_test_rescaled = scaler.fit_transform(X_test)\n",
    "X_test = pd.DataFrame(data = X_test_rescaled, columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, batch_size=100,\n",
       "             hidden_layer_sizes=(23, 17, 13), learning_rate_init=0.4,\n",
       "             max_iter=500, random_state=0, solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, batch_size=100,\n",
       "             hidden_layer_sizes=(23, 17, 13), learning_rate_init=0.4,\n",
       "             max_iter=500, random_state=0, solver=&#x27;sgd&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(activation='logistic', batch_size=100,\n",
       "             hidden_layer_sizes=(23, 17, 13), learning_rate_init=0.4,\n",
       "             max_iter=500, random_state=0, solver='sgd')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = (23, 17, 13)\n",
    "learning_rate = 0.4\n",
    "epochs = 500\n",
    "\n",
    "mlp = MLPRegressor(solver='sgd', random_state = 0, activation = 'logistic', learning_rate_init = learning_rate, batch_size = 100, hidden_layer_sizes = num_nodes, max_iter = epochs)\n",
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.34\n",
      "Testing MSE: 0.35\n",
      "Training R2: -48263799516023113679466659840.00\n",
      "Test R2: -48766868715701755468434112512.00\n"
     ]
    }
   ],
   "source": [
    "y_train_pred =  mlp.predict(X_train)\n",
    "y_test_pred = mlp.predict(X_test)\n",
    "\n",
    "print(\"Training MSE: %.2f\" % (mean_squared_error(y_train_pred, Y_train)))\n",
    "print(\"Testing MSE: %.2f\" % (mean_squared_error(y_test_pred, Y_test)))\n",
    "\n",
    "r2 = r2_score(y_train_pred, Y_train)\n",
    "print('Training R2: %.2f' % r2)\n",
    "r2 = r2_score(y_test_pred, Y_test)\n",
    "print('Test R2: %.2f' % r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very poor performance, let's try swapping the solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=(100, 50), random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=(100, 50), random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(100, 50), random_state=42)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)\n",
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.00044394685388621024\n",
      "Test R2: 1.00\n",
      "Mean Squared Error: 0.030363943525519414\n",
      "Test R2: 0.91\n"
     ]
    }
   ],
   "source": [
    "# training errors\n",
    "y_pred = mlp.predict(X_train)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(Y_train, y_pred))\n",
    "print('Test R2: %.2f' % r2_score(y_pred, Y_train))\n",
    "\n",
    "# testing errors\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(Y_test, y_pred))\n",
    "print('Test R2: %.2f' % r2_score(y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extremely good prediction rate and seems too good to be true. We will verify by using a train, test, validation split to further check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one hot encoding\n",
    "cats = ['Month', 'Day', 'Hour']\n",
    "Xtrain = pd.get_dummies(Xtrain, columns=cats)\n",
    "Xtrain = Xtrain.astype(float)\n",
    "Xtest = pd.get_dummies(Xtest, columns=cats)\n",
    "Xtest = Xtest.astype(float)\n",
    "Xval = pd.get_dummies(Xval, columns=cats)\n",
    "Xval = Xval.astype(float)\n",
    "# print(X)\n",
    "\n",
    "# normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Xtrain_rescaled = scaler.fit_transform(Xtrain)\n",
    "Xtrain = pd.DataFrame(data = Xtrain_rescaled, columns = Xtrain.columns)\n",
    "\n",
    "Xtest_rescaled = scaler.fit_transform(Xtest)\n",
    "Xtest = pd.DataFrame(data = Xtest_rescaled, columns = Xtest.columns)\n",
    "\n",
    "Xval_rescaled = scaler.fit_transform(Xval)\n",
    "Xval = pd.DataFrame(data = Xval_rescaled, columns = Xval.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0007115272536880937\n",
      "Test R2: 1.00\n",
      "Mean Squared Error: 0.04937255760290731\n",
      "Test R2: 0.86\n",
      "Mean Squared Error: 0.010193607408828922\n",
      "Test R2: 0.97\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)\n",
    "mlp.fit(Xtrain, Ytrain)\n",
    "\n",
    "# training errors\n",
    "y_pred = mlp.predict(Xtrain)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(Ytrain, y_pred))\n",
    "print('Test R2: %.2f' % r2_score(y_pred, Ytrain))\n",
    "\n",
    "# testing errors\n",
    "y_pred = mlp.predict(Xtest)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(Ytest, y_pred))\n",
    "print('Test R2: %.2f' % r2_score(y_pred, Ytest))\n",
    "\n",
    "# validation errors\n",
    "y_pred = mlp.predict(Xval)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(Yval, y_pred))\n",
    "print('Test R2: %.2f' % r2_score(y_pred, Yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Near-perfect error holds for validation set too so we can trust that this model performs well. We know it is not overfitting because the testing R2 is just as high as the training value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecs171",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
